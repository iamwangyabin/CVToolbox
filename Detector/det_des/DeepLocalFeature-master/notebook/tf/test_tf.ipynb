{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '../models'\n",
    "if MODEL_PATH not in sys.path:\n",
    "    sys.path.append(MODEL_PATH)\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "from common.tf_layer_utils import *\n",
    "from common.tf_train_utils import get_activation_fn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.argparse_utils import *\n",
    "parser = get_parser()\n",
    "\n",
    "general_arg = add_argument_group('General', parser)\n",
    "general_arg.add_argument('--num_threads', type=int, default=8,\n",
    "                        help='the number of threads (for dataset)')\n",
    "\n",
    "io_arg = add_argument_group('In/Out', parser)\n",
    "io_arg.add_argument('--in_dir', type=str, default='../samples',\n",
    "                        help='input image directory')\n",
    "# io_arg.add_argument('--in_dir', type=str, default='./release/outdoor_examples/images/sacre_coeur/dense/images',\n",
    "#                         help='input image directory')\n",
    "io_arg.add_argument('--out_dir', type=str, default='../dump_feats',\n",
    "                        help='where to save keypoints')\n",
    "io_arg.add_argument('--full_output', type=str2bool, default=True,\n",
    "                        help='dump keypoint image')\n",
    "\n",
    "model_arg = add_argument_group('Model', parser)\n",
    "model_arg.add_argument('--model', type=str, default='../release/models/outdoor/',\n",
    "                        help='model file or directory')\n",
    "model_arg.add_argument('--top_k', type=int, default=500,\n",
    "                        help='number of keypoints')\n",
    "model_arg.add_argument('--max_longer_edge', type=int, default=640,\n",
    "                        help='resize image (do nothing if max_longer_edge <= 0)')\n",
    "\n",
    "tmp_config, unparsed = get_config(parser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore other hyperparams to build model\n",
    "if os.path.isdir(tmp_config.model):\n",
    "    config_path = os.path.join(tmp_config.model, 'config.pkl')\n",
    "else:\n",
    "    config_path = os.path.join(os.path.dirname(tmp_config.model), 'config.pkl')\n",
    "try:\n",
    "    with open(config_path, 'rb') as f:\n",
    "        config = pickle.load(f)\n",
    "except:\n",
    "    raise ValueError('Fail to open {}'.format(config_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for attr, dst_val in sorted(vars(tmp_config).items()):\n",
    "        if hasattr(config, attr):\n",
    "            src_val = getattr(config, attr)\n",
    "            if src_val != dst_val:\n",
    "                setattr(config, attr, dst_val)\n",
    "        else:\n",
    "            setattr(config, attr, dst_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tqdm' from '/home/wang/.conda/envs/py3-tf/lib/python3.5/site-packages/tqdm/__init__.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tqdm\n",
    "from imageio import imread, imsave\n",
    "import cv2\n",
    "tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_paths = [x.path for x in os.scandir(config.in_dir) if x.name.endswith('.jpg') or x.name.endswith('.png')]\n",
    "len(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 477, 640, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "photo = imread(img_paths[0])\n",
    "height, width = photo.shape[:2]\n",
    "longer_edge = max(height, width)\n",
    "if config.max_longer_edge > 0 and longer_edge > config.max_longer_edge:\n",
    "    if height > width:\n",
    "        new_height = config.max_longer_edge\n",
    "        new_width = int(width * config.max_longer_edge / height)\n",
    "    else:\n",
    "        new_height = int(height * config.max_longer_edge / width)\n",
    "        new_width = config.max_longer_edge\n",
    "    photo = cv2.resize(photo, (new_width, new_height))\n",
    "    height, width = photo.shape[:2]\n",
    "rgb = photo.copy()\n",
    "if photo.ndim == 3 and photo.shape[-1] == 3:\n",
    "    photo = cv2.cvtColor(photo, cv2.COLOR_RGB2GRAY)\n",
    "photo = photo[None,...,None].astype(np.float32) / 255.0 # normalize 0-1\n",
    "assert photo.ndim == 4 # [1,H,W,1]\n",
    "\n",
    "photo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 477, 640, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "photo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wshape = [3, 3,1, 64]\n",
    "bshape = 64\n",
    "initializer = tf.contrib.layers.xavier_initializer()\n",
    "bshape = [bshape]\n",
    "W = tf.get_variable('weights', wshape, initializer=initializer, dtype=tf.float32)\n",
    "b = tf.get_variable('biases', bshape, initializer=tf.zeros_initializer(), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder('float32',[1,None,None,1])\n",
    "strides = [1, 2, 2, 1]\n",
    "curr= tf.nn.conv2d(x, W,strides,padding='SAME')\n",
    "add_bias=tf.nn.bias_add(curr, b)\n",
    "\n",
    "init_op=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init_op.run()\n",
    "    M_conv=sess.run(add_bias,feed_dict={x:photo})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 239, 320, 64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_conv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是ｄｅｔｅｃｔｏｒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Pad:0' shape=(1, 481, 644, 1) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_total = 5 - 1\n",
    "pad_beg = pad_total // 2\n",
    "pad_end = pad_total - pad_beg\n",
    "\n",
    "padded_inputs = tf.pad(photo, [[0, 0], [pad_beg, pad_end],\n",
    "                            [pad_beg, pad_end], [0, 0]])\n",
    "padded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wshape = [5, 5,1, 16]\n",
    "bshape = 16\n",
    "initializer2 = tf.contrib.layers.xavier_initializer()\n",
    "bshape = [bshape]\n",
    "W1 = tf.get_variable('weisdas', wshape, initializer=initializer2, dtype=tf.float32)\n",
    "b1 = tf.get_variable('bassad', bshape, initializer=tf.zeros_initializer(), dtype=tf.float32)\n",
    "\n",
    "x=tf.placeholder('float32',[1,None,None,1])\n",
    "strides = [1, 1, 1, 1]\n",
    "curr= tf.nn.conv2d(x, W1,strides,padding='SAME')\n",
    "add_bias=tf.nn.bias_add(curr, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_op=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init_op.run()\n",
    "    M_conv2=sess.run(add_bias,feed_dict={x:photo})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 477, 640, 16)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_conv2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_height_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-698504cdd287>:4: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n",
      "[[0.26726124 0.5345225  0.8017837 ]\n",
      " [0.45584232 0.5698029  0.6837635 ]\n",
      " [0.5025707  0.5743665  0.64616233]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "input_data = tf.constant([[1.0,2,3],[4.0,5,6],[7.0,8,9]])\n",
    "\n",
    "output = tf.nn.l2_normalize(input_data, dim = -1)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(input_data))\n",
    "    print(sess.run(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'random_uniform:0' shape=(2,) dtype=float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random_uniform([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = tf.random_uniform([2])\n",
    "\n",
    "output = tf.nn.l2_normalize(input_data, dim = -1)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(input_data))\n",
    "    print(sess.run(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
