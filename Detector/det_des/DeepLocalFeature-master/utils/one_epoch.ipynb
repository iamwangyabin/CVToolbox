{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import importlib\n",
    "import time\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import DatasetsBuilder\n",
    "\n",
    "import getConfig\n",
    "config,unparsed=getConfig.get_lfconfig()\n",
    "\n",
    "import buildTrainNetwork\n",
    "import det_tools\n",
    "from inference import *\n",
    "\n",
    "from common.tf_layer_utils import *\n",
    "from common.tf_train_utils import get_optimizer, get_piecewise_lr, get_activation_fn\n",
    "from common.tfvisualizer import log_images, convert_tile_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../dataset/scan/train.tfrecord']\n",
      "aug_mode=random max_rad=3.141592653589793, max_scale_log=0.34642256747438094\n",
      "Add random logscale=-0.35~0.35, ori=-3.141592653589793~3.141592653589793\n"
     ]
    }
   ],
   "source": [
    "# tf.reset_default_graph() # for sure\n",
    "\n",
    "log_dir = config.log_dir\n",
    "batch_size = config.batch_size\n",
    "optim_method = config.optim_method\n",
    "learning_rate = config.lr\n",
    "va_batch_size = 1\n",
    "\n",
    "tr_loader = DatasetsBuilder.SfMDataset(out_size=(362, 362), \n",
    "                   warp_aug_mode='random', flip_pair=True, max_degree=180, max_scale=1.414,\n",
    "                   num_threads=2)\n",
    "tr_dataset = tr_loader.get_dataset('../dataset', 'config.sfm_img_dir', \n",
    "                    'scan', phase='train',\n",
    "                    batch_size=6, shuffle=True)\n",
    "# config.depth_thresh = tr_loader.depth_thresh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = tr_dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "next_batch = list(next_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice_1:0\", shape=(?, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "next_batch = DatasetsBuilder.euclidean_augmentation(next_batch, (config.data_size, config.data_size), config.rot_aug, config.scale_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training_ph = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "psf = tf.constant(det_tools.get_gauss_filter_weight(config.hm_ksize, config.hm_sigma)[:,:,None,None], dtype=tf.float32) \n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply instance norm on input photos\n",
      "Act-Fn:  <function get_activation_fn.<locals>.<lambda> at 0x7f91f6bd9510>\n",
      "Apply 3D NMS instead.\n",
      "Scales (0.707107~1.41 #5): [1.41421356 1.18920712 1.         0.84089642 0.70710678]\n",
      "WARNING:tensorflow:From ../models/mso_resnet_detector.py:148: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "INFO:tensorflow:Summary name ConvOnlyResNet/ori_conv/weights:0 is illegal; using ConvOnlyResNet/ori_conv/weights_0 instead.\n",
      "INFO:tensorflow:Summary name ConvOnlyResNet/ori_conv/biases:0 is illegal; using ConvOnlyResNet/ori_conv/biases_0 instead.\n",
      "PAD=16, #conv=8, ksize=5 ori-ksize=5\n",
      "WARNING:tensorflow:From /home/wang/workspace/DeepLD/utils/det_tools.py:1729: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/wang/workspace/DeepLD/utils/det_tools.py:1730: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Scales (0.707107~1.41 #5): [1.41421356 1.18920712 1.         0.84089642 0.70710678]\n",
      "PAD=16, #conv=8, ksize=5 ori-ksize=5\n",
      "Act-Fn:  <function relu at 0x7f920da089d8>\n",
      "===== SimpleDesc (reuse=False) =====\n",
      "#1 conv-bn-act (?, 16, 16, 64)\n",
      "#2 conv-bn-act (?, 8, 8, 128)\n",
      "#3 conv-bn-act (?, 4, 4, 256)\n",
      "FLAT (?, 4096)\n",
      "Feat-Norm: L2-NORM\n",
      "FEAT (?, 256)\n",
      "===== SimpleDesc (reuse=True) =====\n",
      "#1 conv-bn-act (?, 16, 16, 64)\n",
      "#2 conv-bn-act (?, 8, 8, 128)\n",
      "#3 conv-bn-act (?, 4, 4, 256)\n",
      "FLAT (?, 4096)\n",
      "Feat-Norm: L2-NORM\n",
      "FEAT (?, 256)\n",
      "===== SimpleDesc (reuse=True) =====\n",
      "#1 conv-bn-act (?, 16, 16, 64)\n",
      "#2 conv-bn-act (?, 8, 8, 128)\n",
      "#3 conv-bn-act (?, 4, 4, 256)\n",
      "FLAT (?, 4096)\n",
      "Feat-Norm: L2-NORM\n",
      "FEAT (?, 256)\n",
      "Random Hard Mining with scheduling #pickup=64-->5 (decay=0.9)\n"
     ]
    }
   ],
   "source": [
    "det_loss, desc_loss, det_endpoints, desc_endpoints, eval_endpoints, sift_endpoints = \\\n",
    "                    buildTrainNetwork.build_training_network(config, next_batch, is_training_ph, psf, global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_var_list = det_endpoints['var_list'] + det_endpoints['mso_var_list']\n",
    "desc_var_list = desc_endpoints['var_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== get_optimizer (adam) ==========\n",
      "<tensorflow.python.training.adam.AdamOptimizer object at 0x7f91ed9d5e10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wang/.conda/envs/py3-tf/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleDesc/conv1/weights:0 (3, 3, 1, 64)\n",
      "SimpleDesc/conv1/biases:0 (64,)\n",
      "SimpleDesc/bn1/gamma:0 (64,)\n",
      "SimpleDesc/bn1/beta:0 (64,)\n",
      "SimpleDesc/conv2/weights:0 (3, 3, 64, 128)\n",
      "SimpleDesc/conv2/biases:0 (128,)\n",
      "SimpleDesc/bn2/gamma:0 (128,)\n",
      "SimpleDesc/bn2/beta:0 (128,)\n",
      "SimpleDesc/conv3/weights:0 (3, 3, 128, 256)\n",
      "SimpleDesc/conv3/biases:0 (256,)\n",
      "SimpleDesc/bn3/gamma:0 (256,)\n",
      "SimpleDesc/bn3/beta:0 (256,)\n",
      "SimpleDesc/fc1/weights:0 (4096, 512)\n",
      "SimpleDesc/fc1/biases:0 (512,)\n",
      "SimpleDesc/fc-bn1/gamma:0 (512,)\n",
      "SimpleDesc/fc-bn1/beta:0 (512,)\n",
      "SimpleDesc/fc2/weights:0 (512, 256)\n",
      "SimpleDesc/fc2/biases:0 (256,)\n",
      "=======================================\n",
      "========== get_optimizer (adam) ==========\n",
      "<tensorflow.python.training.adam.AdamOptimizer object at 0x7f91ecfd1fd0>\n",
      "ConvOnlyResNet/init_conv/weights:0 (5, 5, 1, 16)\n",
      "ConvOnlyResNet/init_conv/biases:0 (16,)\n",
      "ConvOnlyResNet/block-1/pre-bn/gamma:0 (16,)\n",
      "ConvOnlyResNet/block-1/pre-bn/beta:0 (16,)\n",
      "ConvOnlyResNet/block-1/conv1/weights:0 (5, 5, 16, 16)\n",
      "ConvOnlyResNet/block-1/conv1/biases:0 (16,)\n",
      "ConvOnlyResNet/block-1/mid-bn/gamma:0 (16,)\n",
      "ConvOnlyResNet/block-1/mid-bn/beta:0 (16,)\n",
      "ConvOnlyResNet/block-1/conv2/weights:0 (5, 5, 16, 16)\n",
      "ConvOnlyResNet/block-1/conv2/biases:0 (16,)\n",
      "ConvOnlyResNet/block-2/pre-bn/gamma:0 (16,)\n",
      "ConvOnlyResNet/block-2/pre-bn/beta:0 (16,)\n",
      "ConvOnlyResNet/block-2/conv1/weights:0 (5, 5, 16, 16)\n",
      "ConvOnlyResNet/block-2/conv1/biases:0 (16,)\n",
      "ConvOnlyResNet/block-2/mid-bn/gamma:0 (16,)\n",
      "ConvOnlyResNet/block-2/mid-bn/beta:0 (16,)\n",
      "ConvOnlyResNet/block-2/conv2/weights:0 (5, 5, 16, 16)\n",
      "ConvOnlyResNet/block-2/conv2/biases:0 (16,)\n",
      "ConvOnlyResNet/block-3/pre-bn/gamma:0 (16,)\n",
      "ConvOnlyResNet/block-3/pre-bn/beta:0 (16,)\n",
      "ConvOnlyResNet/block-3/conv1/weights:0 (5, 5, 16, 16)\n",
      "ConvOnlyResNet/block-3/conv1/biases:0 (16,)\n",
      "ConvOnlyResNet/block-3/mid-bn/gamma:0 (16,)\n",
      "ConvOnlyResNet/block-3/mid-bn/beta:0 (16,)\n",
      "ConvOnlyResNet/block-3/conv2/weights:0 (5, 5, 16, 16)\n",
      "ConvOnlyResNet/block-3/conv2/biases:0 (16,)\n",
      "ConvOnlyResNet/fin-bn/gamma:0 (16,)\n",
      "ConvOnlyResNet/fin-bn/beta:0 (16,)\n",
      "ConvOnlyResNet/score_conv_0/weights:0 (5, 5, 16, 1)\n",
      "ConvOnlyResNet/score_conv_0/biases:0 (1,)\n",
      "ConvOnlyResNet/score_conv_1/weights:0 (5, 5, 16, 1)\n",
      "ConvOnlyResNet/score_conv_1/biases:0 (1,)\n",
      "ConvOnlyResNet/score_conv_2/weights:0 (5, 5, 16, 1)\n",
      "ConvOnlyResNet/score_conv_2/biases:0 (1,)\n",
      "ConvOnlyResNet/score_conv_3/weights:0 (5, 5, 16, 1)\n",
      "ConvOnlyResNet/score_conv_3/biases:0 (1,)\n",
      "ConvOnlyResNet/score_conv_4/weights:0 (5, 5, 16, 1)\n",
      "ConvOnlyResNet/score_conv_4/biases:0 (1,)\n",
      "ConvOnlyResNet/ori_conv/weights:0 (5, 5, 16, 2)\n",
      "ConvOnlyResNet/ori_conv/biases:0 (2,)\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "desc_minimize_op = get_optimizer(optim_method, global_step, learning_rate, desc_loss, desc_var_list, show_var_and_grad=config.show_histogram)\n",
    "# detector minimizer\n",
    "det_minimize_op = get_optimizer(optim_method, global_step, learning_rate, det_loss, det_var_list, show_var_and_grad=config.show_histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfconfig = tf.ConfigProto()\n",
    "tfconfig.gpu_options.allow_growth = True # almost the same as tf.InteractiveSession\n",
    "sess = tf.Session(config=tfconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "tr_iter = tr_dataset.make_one_shot_iterator() # infinite loop\n",
    "\n",
    "tr_handle = sess.run(tr_iter.string_handle())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_saver = tf.train.Saver(max_to_keep=1, save_relative_paths=True)\n",
    "# # latest_saver = tf.train.Saver(max_to_keep=1, save_relative_paths=True)\n",
    "# latest_saver = tf.train.Saver(max_to_keep=100, save_relative_paths=True) # save everything\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
