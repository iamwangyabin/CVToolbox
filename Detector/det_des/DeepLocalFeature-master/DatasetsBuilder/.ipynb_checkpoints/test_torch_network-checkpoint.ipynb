{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "# import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5578"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imageio import imread, imsave\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "in_dir='../dataset/color/'\n",
    "img_paths = [x.path for x in os.scandir(in_dir) if x.name.endswith('.jpg') or x.name.endswith('.png')]\n",
    "len(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo = imread(img_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import PIL \n",
    "\n",
    "# plt.figure(\"dog\")\n",
    "# plt.imshow(photo)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成一个图片载入程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "imsize = 256\n",
    "loader = torchvision.transforms.Compose([\n",
    "#     transforms.Scale(imsize), \n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def image_loader(image_name):\n",
    "    \"\"\"load image, returns cuda tensor\"\"\"\n",
    "    image = PIL.Image.open(image_name)\n",
    "    image = loader(image).float()\n",
    "    image = Variable(image, requires_grad=True)\n",
    "    image = image.unsqueeze(0)  #this is for VGG, may not be needed for ResNet\n",
    "    return image #assumes that you're using GPU\n",
    "\n",
    "image = image_loader(img_paths[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 968, 1296])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义Detector网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, use_bias=True,downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.bn0=nn.BatchNorm2d(inplanes)\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=5, stride=stride,padding=2, bias=use_bias)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=5, stride=stride,padding=2, bias=use_bias)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.bn0(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectorModel(torch.nn.Module):\n",
    "    def __init__(self, num_block=3, num_channels=16,conv_ksize=5,\n",
    "                 use_bias=True, min_scale=2**-3, max_scale=1, num_scales=9):\n",
    "\n",
    "        self.inplanes = num_channels\n",
    "        self.num_blocks=num_block\n",
    "        self.min_scale = min_scale\n",
    "        self.max_scale=max_scale\n",
    "        self.num_scales=num_scales\n",
    "\n",
    "        super(DetectorModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=conv_ksize, stride=1, padding=2,\n",
    "                               bias=use_bias)\n",
    "        self.bn1 = nn.BatchNorm2d(self.inplanes)\n",
    "        self.layer=BasicBlock(self.inplanes, self.inplanes, stride=1, use_bias=True)\n",
    "        self.soft_conv=nn.Conv2d(16, 1, kernel_size=conv_ksize, stride=1, padding=2,\n",
    "                               bias=use_bias)\n",
    "        self.ori_layer=nn.Conv2d(self.inplanes,2,kernel_size=conv_ksize, stride=1, padding=2,\n",
    "                                bias=True )\n",
    "#         ori_b_init=torch.nn.init.constant(np.array([1,0], dtype=np.float32))\n",
    "#         self.ori_layer.bias.data.fill_(ori_b_init)\n",
    "        if self.num_scales == 1:\n",
    "            self.scale_factors = [1.0]\n",
    "        else:\n",
    "            scale_log_factors = np.linspace(np.log(self.max_scale), np.log(self.min_scale), self.num_scales)\n",
    "            self.scale_factors = np.exp(scale_log_factors)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.conv1(x)\n",
    "        for i in range(self.num_blocks):\n",
    "            x=self.layer(x)\n",
    "            print(1)\n",
    "        x=self.bn1(x)\n",
    "        score_maps_list = []\n",
    "        base_height_f = x.shape[2]\n",
    "        base_width_f = x.shape[3]\n",
    "        for i, s in enumerate(self.scale_factors):\n",
    "            feat_height = (base_height_f * s + 0.5).astype(np.uint32)\n",
    "            feat_width = (base_width_f * s + 0.5).astype(np.uint32)\n",
    "            rs_feat_maps=torch.nn.functional.interpolate(x,[feat_height, feat_width])\n",
    "            score_maps = self.soft_conv(rs_feat_maps)\n",
    "            score_maps_list.append(score_maps)\n",
    "#         ori_b_init=torch.nn.init.constant(np.array([1,0], dtype=np.float32))\n",
    "#         self.ori_layer.bias.data.fill_(ori_b_init)\n",
    "        ori_maps=self.ori_layer(x)\n",
    "        norm = ori_maps.norm(p=2, dim=1, keepdim=True)\n",
    "        ori_maps = ori_maps.div(norm.expand_as(ori_maps))\n",
    "    \n",
    "        endpoints={}\n",
    "        endpoints['ori_maps'] = ori_maps\n",
    "        endpoints['scale_factors'] = self.scale_factors\n",
    "        return score_maps_list,endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DetectorModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(),lr=0.001)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "score_maps_list,endpoints = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 968, 1296])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_maps_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 968, 1296])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoints['ori_maps'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可以看出来网络输入输出都是一样的尺寸,这样子根据网络输出可以直接得到想要的特征值位置，尺度，方向．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0640, -0.2276,  0.1485,  ..., -0.9811,  0.3224, -0.8869],\n",
       "          [ 0.3082, -0.0917,  0.0193,  ..., -0.3908,  0.3010, -0.7215],\n",
       "          [ 0.4699,  0.6957,  0.9656,  ...,  0.5958, -0.1470, -0.1650],\n",
       "          ...,\n",
       "          [ 0.5943,  0.7603,  0.9874,  ..., -0.8233, -0.3601, -0.4108],\n",
       "          [ 0.9684,  0.3985,  0.5128,  ...,  0.7687,  0.4237,  0.6660],\n",
       "          [-0.9906, -0.8950,  0.3963,  ...,  0.4654, -0.5938,  0.9230]],\n",
       "\n",
       "         [[ 0.9979,  0.9738,  0.9889,  ...,  0.1937, -0.9466, -0.4620],\n",
       "          [ 0.9513,  0.9958,  0.9998,  ...,  0.9205, -0.9536, -0.6924],\n",
       "          [ 0.8827,  0.7183,  0.2600,  ..., -0.8031,  0.9891,  0.9863],\n",
       "          ...,\n",
       "          [ 0.8042,  0.6496, -0.1583,  ..., -0.5676, -0.9329, -0.9117],\n",
       "          [ 0.2494,  0.9172,  0.8585,  ..., -0.6396, -0.9058, -0.7459],\n",
       "          [ 0.1371, -0.4461,  0.9181,  ..., -0.8851, -0.8046,  0.3848]]]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoints['ori_maps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 968, 1296])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoints['ori_maps'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3307, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoints['ori_maps'][0][0][123][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9437, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoints['ori_maps'][0][1][123][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99993218"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.3307**2+0.9437**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2的目的就是让ori map输出的值是一个真实的角度值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义Descriptor网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Descriptor(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "            out_dim=128,init_num_channels=64,\n",
    "            num_conv_layers=3,use_bias=False,\n",
    "            conv_ksize=3):\n",
    "        super(Descriptor, self).__init__()\n",
    "        in_channel=2\n",
    "        channels_list = [init_num_channels * 2 ** i for i in range(num_conv_layers)]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, channels_list[0], kernel_size=conv_ksize, stride=2,padding=1, bias=use_bias)\n",
    "        self.bn1 = nn.BatchNorm2d(channels_list[0])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(channels_list[0], channels_list[1], kernel_size=conv_ksize, stride=2,padding=1, bias=use_bias)\n",
    "        self.bn2 = nn.BatchNorm2d(channels_list[1])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv3 = nn.Conv2d(channels_list[1], channels_list[2], kernel_size=conv_ksize, stride=2, padding=1, bias=use_bias)\n",
    "        self.bn2 = nn.BatchNorm2d(channels_list[2])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(512 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, out_dim)\n",
    "\n",
    "        # ori_maps = f.normalize(ori_maps, dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.bn0(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
